{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUANWN3rpfC9"
   },
   "source": [
    "# Import\n",
    "Importieren der erforderlichen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "146BB11JpfDA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.2\n"
     ]
    }
   ],
   "source": [
    "# optionale Ausgabe der Python-Version, diese sollte 3.6.8 sein\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisierung der Ordnerstruktur\n",
    "In diesem Abschnitt werden die Pfade zu den erforderlichen Ordnern und Dateien initialisiert. Hier wird auch der Name des später erstellten Modells festgelegt. Anschließend werdennicht existierende Ordner Angelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hbPhYVy_pfDB"
   },
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'MY_FIRST_MODEL' #Hier den Namen für das  zu erstellende Modell eingeben\n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'#Name des Heruntergeladenen Modells und Link hierzu\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n",
    " }\n",
    "\n",
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HR-TfDGrpfDC"
   },
   "outputs": [],
   "source": [
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        !mkdir {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLU-rs_ipfDE"
   },
   "source": [
    "# Herunterladen der Vortrainierten Modelle und der Protocolbuffers\n",
    "Zuerst werden die Vortrainierten Modelle heruntergeladen und in der zuvor angelegten Ordnerstruktur abgelegt. Anschließend werden die Protocolbuffers heruntergeladenmund installiert. Da somit alle Vorbereitungen abgeschlossen sind, kann nun das Verfifikations-script ausgeführt werden. Erscheint dort \"OK\", kann mit den Nächsten Schritten forgefahren werden. Es ist allerdings möglich, dass noch weitere Bibliotheken benötigt werden. Diese müssen dann installiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iA1DIq5OpfDE",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Tensorflow\\models'...\n",
      "Updating files:  36% (1159/3203)\n",
      "Updating files:  37% (1186/3203)\n",
      "Updating files:  38% (1218/3203)\n",
      "Updating files:  39% (1250/3203)\n",
      "Updating files:  40% (1282/3203)\n",
      "Updating files:  41% (1314/3203)\n",
      "Updating files:  42% (1346/3203)\n",
      "Updating files:  43% (1378/3203)\n",
      "Updating files:  44% (1410/3203)\n",
      "Updating files:  45% (1442/3203)\n",
      "Updating files:  46% (1474/3203)\n",
      "Updating files:  47% (1506/3203)\n",
      "Updating files:  48% (1538/3203)\n",
      "Updating files:  49% (1570/3203)\n",
      "Updating files:  50% (1602/3203)\n",
      "Updating files:  51% (1634/3203)\n",
      "Updating files:  52% (1666/3203)\n",
      "Updating files:  53% (1698/3203)\n",
      "Updating files:  54% (1730/3203)\n",
      "Updating files:  55% (1762/3203)\n",
      "Updating files:  56% (1794/3203)\n",
      "Updating files:  57% (1826/3203)\n",
      "Updating files:  58% (1858/3203)\n",
      "Updating files:  59% (1890/3203)\n",
      "Updating files:  60% (1922/3203)\n",
      "Updating files:  61% (1954/3203)\n",
      "Updating files:  62% (1986/3203)\n",
      "Updating files:  63% (2018/3203)\n",
      "Updating files:  63% (2031/3203)\n",
      "Updating files:  64% (2050/3203)\n",
      "Updating files:  65% (2082/3203)\n",
      "Updating files:  66% (2114/3203)\n",
      "Updating files:  67% (2147/3203)\n",
      "Updating files:  68% (2179/3203)\n",
      "Updating files:  69% (2211/3203)\n",
      "Updating files:  70% (2243/3203)\n",
      "Updating files:  71% (2275/3203)\n",
      "Updating files:  72% (2307/3203)\n",
      "Updating files:  73% (2339/3203)\n",
      "Updating files:  74% (2371/3203)\n",
      "Updating files:  75% (2403/3203)\n",
      "Updating files:  76% (2435/3203)\n",
      "Updating files:  77% (2467/3203)\n",
      "Updating files:  78% (2499/3203)\n",
      "Updating files:  79% (2531/3203)\n",
      "Updating files:  80% (2563/3203)\n",
      "Updating files:  81% (2595/3203)\n",
      "Updating files:  82% (2627/3203)\n",
      "Updating files:  83% (2659/3203)\n",
      "Updating files:  84% (2691/3203)\n",
      "Updating files:  85% (2723/3203)\n",
      "Updating files:  86% (2755/3203)\n",
      "Updating files:  87% (2787/3203)\n",
      "Updating files:  88% (2819/3203)\n",
      "Updating files:  89% (2851/3203)\n",
      "Updating files:  89% (2856/3203)\n",
      "Updating files:  90% (2883/3203)\n",
      "Updating files:  91% (2915/3203)\n",
      "Updating files:  92% (2947/3203)\n",
      "Updating files:  93% (2979/3203)\n",
      "Updating files:  94% (3011/3203)\n",
      "Updating files:  95% (3043/3203)\n",
      "Updating files:  96% (3075/3203)\n",
      "Updating files:  97% (3107/3203)\n",
      "Updating files:  98% (3139/3203)\n",
      "Updating files:  99% (3171/3203)\n",
      "Updating files: 100% (3203/3203)\n",
      "Updating files: 100% (3203/3203), done.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection')):\n",
    "    !git clone https://github.com/tensorflow/models {paths['APIMODEL_PATH']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJjMHbnDs3Tv"
   },
   "outputs": [],
   "source": [
    "# Install Tensorflow Object Detection \n",
    "url=\"https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip\"\n",
    "wget.download(url)\n",
    "!move protoc-3.15.6-win64.zip {paths['PROTOC_PATH']}\n",
    "!cd {paths['PROTOC_PATH']} && tar -xf protoc-3.15.6-win64.zip\n",
    "os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['PROTOC_PATH'], 'bin'))\n",
    "!cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. \n",
    "!cd Tensorflow/models/research && copy object_detection\\\\packages\\\\tf2\\\\setup.py . \n",
    "!cd Tensorflow/models/research && python setup.py build \n",
    "!cd Tensorflow/models/research && python setup.py install\n",
    "!cd Tensorflow/models/research/slim \n",
    "!cd Tensorflow/models/research && pip install -e . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "# Verify Installation\n",
    "!python {VERIFICATION_SCRIPT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neustart des Kernels\n",
    "Nachdem die Protocolbuffers intalliert sind, ist ein Neustart des Kernels erforderlich. Anschließend wird hier das Vortrainierte Modell in den entsprechenden Ordner verschoben. Danach müssen wieder alle Bibliotheken importiert und alle Initialisierungen erneut erfolgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csofht2npfDE",
    "outputId": "ff5471b2-bed2-43f2-959c-327a706527b6"
   },
   "outputs": [],
   "source": [
    "wget.download(PRETRAINED_MODEL_URL)\n",
    "!move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "!cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5KJTnkfpfDC"
   },
   "source": [
    "# Erstellen der Label Map\n",
    "Die Label Map beihaltet alle zu erkennenden Objekte mit einer zugewiesenen ID. Sie ist für das Trainieren und die spätere Erkennung wichtig. Die aus dem Neuronalen Netzwerk stammenden ID´s werden hier den Labels zugewiesen. Diese müssen mit den Labels beim Bilder vorbereiten übereinstimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1BVDWo7pfDC"
   },
   "outputs": [],
   "source": [
    "# Beispiel: labels = [{'name':'Objekt_1', 'id':1},{'name':'Objekt_2', 'id':2},{'name':'Objekt_3', 'id':3}]\n",
    "labels = [{'name':'WT_001', 'id':1}]\n",
    "\n",
    "with open(files['LABELMAP'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C88zyVELpfDC"
   },
   "source": [
    "# Erstellen der TF records\n",
    "Das erstellen der Test und Train Records erfolgt durch das hier zuerst heruntergeladene \"TF_RECORD_SCRIPT\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWpb_BVUpfDD",
    "outputId": "56ce2a3f-3933-4ee6-8a9d-d5ec65f7d73c"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(files['TF_RECORD_SCRIPT']):\n",
    "    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths['SCRIPTS_PATH']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPFToGZqpfDD",
    "outputId": "0ebb456f-aadc-4a1f-96e6-fbfec1923e1c"
   },
   "outputs": [],
   "source": [
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'train')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')} \n",
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'test')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT4QU7pLpfDE"
   },
   "source": [
    "# Konfigurieren der Pipeline\n",
    "Nachdem die Pipline Datei des Vortrainierten Modells für das neue Modell kopiert wurde, wird diese Angepasst. Es werden die Label Map und die Record Files benötigt. Es werden auch die erforderlichen Bibliotheken importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOjuTFbwpfDF"
   },
   "outputs": [],
   "source": [
    "!copy {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9hRrO_ppfDF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2A0mn4ipfDF"
   },
   "outputs": [],
   "source": [
    "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vK5lotDpfDF"
   },
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP43Ph0JpfDG"
   },
   "outputs": [],
   "source": [
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJvfgwWqpfDG"
   },
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr3ON7xMpfDG"
   },
   "source": [
    "# Modell trainieren\n",
    "Das Training des Modells erfolgt nachdem alle vorherigen Schritte erfolgreich abgeschlossen sind. Hier wird die Anzahl der Trainigsschritte durch anpassen des Arguments \"--num_train_steps\" festgelegt. Das Training kann entweder in diesem Notebook erfolgen oder in einem gesonderten Terminal, falls Fortschrittsausgaben während des Trainings erwünscht sind. Dann muss allerdings darauf geachtet werden, dass das richtige Environment aktiviert ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-Y2UQmQpfDG"
   },
   "outputs": [],
   "source": [
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMP2XDfQpfDH"
   },
   "outputs": [],
   "source": [
    "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=2000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4OXXi-ApfDH",
    "outputId": "117a0e83-012b-466e-b7a6-ccaa349ac5ab"
   },
   "outputs": [],
   "source": [
    "# Ausgabe des Trainings-Befehls\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3ZsJR-qpfDH",
    "outputId": "cabec5e1-45e6-4f2f-d9cf-297d9c1d0225"
   },
   "outputs": [],
   "source": [
    "# Ausführen des Trainings\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_YRZu7npfDH"
   },
   "source": [
    "# Evaluierung des Modells\n",
    "In diesem Abschnitt wird die Evaluation des Modells durchgeführt. Dabei kann wie beim vorherigen Punkt das Terminal genutzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80L7-fdPpfDH"
   },
   "outputs": [],
   "source": [
    "command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYsgEPx9pfDH",
    "outputId": "8632d48b-91d2-45d9-bcb8-c1b172bf6eed"
   },
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqTV2jGBpfDH"
   },
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "Nachdem die Evaluierung durchgeführt wurde, kann das Ergebnis mit Tensorboard visualisiert werden. Hierzu muss in den \"eval\" oder \"train\" Ordner des erstellten Modells navigiert werden. Der Pfad ist: .\\Tensorflow\\workspace\\models\\MY_FIRST_MODEL\\test bzw. .\\Tensorflow\\workspace\\models\\MY_FIRST_MODEL\\eval. Wird der Befehl \"Tensorboard\" ausgeführt, wird ein Webserver gestartet, welcher unter der dann ausgegebenen Adresse aufgerufen werden kann."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2.Training and Detection",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "beerDt",
   "language": "python",
   "name": "beerdt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
